一、Hive快捷查询：不启用Mapreduce job启用Fetch task三种方式介绍

    如果你想查询某个表的某一列，Hive默认是会启用MapReduce Job来完成这个任务，如下：
    hive> SELECT id, money FROM m limit 10;
    Total MapReduce jobs = 1
    Launching Job 1 out of 1
    Number of reduce tasks is set to 0 since there's no reduce operator
    Cannot run job locally: Input Size (= 235105473) is larger than
    hive.exec.mode.local.auto.inputbytes.max (= 134217728)
    Starting Job = job_1384246387966_0229, Tracking URL =

    http://l-datalogm1.data.cn1:9981/proxy/application_1384246387966_0229/

    Kill Command = /home/q/hadoop-2.2.0/bin/hadoop job
    -kill job_1384246387966_0229
    hadoop job information for Stage-1: number of mappers: 1;
    number of reducers: 0
    2013-11-13 11:35:16,167 Stage-1 map = 0%,  reduce = 0%
    2013-11-13 11:35:21,327 Stage-1 map = 100%,  reduce = 0%,
    Cumulative CPU 1.26 sec
    2013-11-13 11:35:22,377 Stage-1 map = 100%,  reduce = 0%,
    Cumulative CPU 1.26 sec
    MapReduce Total cumulative CPU time: 1 seconds 260 msec
    Ended Job = job_1384246387966_0229
    MapReduce Jobs Launched:
    Job 0: Map: 1   Cumulative CPU: 1.26 sec
    HDFS Read: 8388865 HDFS Write: 60 SUCCESS
    Total MapReduce CPU Time Spent: 1 seconds 260 msec
    OK
    1       122
    1       185
    1       231
    1       292
    1       316
    1       329
    1       355
    1       356
    1       362
    1       364
    Time taken: 16.802 seconds, Fetched: 10 row(s)

    我们都知道，启用MapReduce Job是会消耗系统开销的。对于这个问题，从Hive0.10.0版本开始，对于简单的不需要聚合的类似
    SELECT <col> from <table> LIMIT n语句，不需要起MapReduce job，直接通过Fetch task获取数据，可以通过下面几种方法实现：
    方法一：
        hive> set hive.fetch.task.conversion=more;
        hive> SELECT id, money FROM m limit 10;
        OK
        1       122
        1       185
        1       231
        1       292
        1       316
        1       329
        1       355
        1       356
        1       362
        1       364
        Time taken: 0.138 seconds, Fetched: 10 row(s)
    上面 set hive.fetch.task.conversion=more;开启了Fetch任务，所以对于上述简单的列查询不在启用MapReduce job！

    方法二：
        bin/hive --hiveconf hive.fetch.task.conversion=more

    方法三：
        上面的两种方法都可以开启了Fetch任务，但是都是临时起作用的；如果你想一直启用这个功能，可以在${HIVE_HOME}/conf/hive-site.xml里面加入以下配置：
        <property>
          <name>hive.fetch.task.conversion</name>
          <value>more</value>
          <description>
            Some select queries can be converted to single FETCH task
            minimizing latency.Currently the query should be single
            sourced not having any subquery and should not have
            any aggregations or distincts (which incurrs RS),
            lateral views and joins.
            1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only
            2. more    : SELECT, FILTER, LIMIT only (+TABLESAMPLE, virtual columns)
          </description>
        </property>

二、Hive如何执行文件中的sql语句

    Hive可以运行保存在文件里面的一条或多条的语句，只要用-f参数，一般情况下，保存这些Hive查询语句的文件通常用.q或者
    .hql后缀名，但是这不是必须的，你也可以保存你想要的后缀名。假设test文件里面有一下的Hive查询语句：
    select * from p limit 10;
    select count(*) from p;

    那么我们可以用下面的命令来查询：
    [wyp@wyp hive-0.11.0-bin]$ bin/hive -f test

    ........这里省略了一些输出...........
    OK
    196        242        3        881250949        20131102        jx
    186        302        3        891717742        20131102        jx
    22        377        1        878887116        20131102        jx
    244        51        2        880606923        20131102        jx
    166        346        1        886397596        20131102        jx
    298        474        4        884182806        20131102        jx
    115        265        2        881171488        20131102        jx
    253        465        5        891628467        20131102        jx
    305        451        3        886324817        20131102        jx
    6        86        3        883603013        20131102        jx
    Time taken: 4.386 seconds, Fetched: 10 row(s)

    ........这里省略了一些输出...........
    OK
    4000000
    Time taken: 16.284 seconds, Fetched: 1 row(s)

    如果你配置好了Hive shell的路径，你可以用SOURCE命令来运行那个查询文件:
    [wyp@wyp hive-0.11.0-bin]$ hive
    hive> source /home/wyp/Documents/test;

    ........这里省略了一些输出...........
    OK
    196        242        3        881250949        20131102        jx
    186        302        3        891717742        20131102        jx
    22        377        1        878887116        20131102        jx
    244        51        2        880606923        20131102        jx
    166        346        1        886397596        20131102        jx
    298        474        4        884182806        20131102        jx
    115        265        2        881171488        20131102        jx
    253        465        5        891628467        20131102        jx
    305        451        3        886324817        20131102        jx
    6        86        3        883603013        20131102        jx
    Time taken: 4.386 seconds, Fetched: 10 row(s)

    ........这里省略了一些输出...........
    OK
    4000000
    Time taken: 16.284 seconds, Fetched: 1 row(s)

三、Hive四种数据导入方式介绍

    Hive的几种常见的数据导入方式
    这里介绍四种：
    （1）、从本地文件系统中导入数据到Hive表；
    （2）、从HDFS上导入数据到Hive表；
    （3）、从别的表中查询出相应的数据并导入到Hive表中；
    （4）、在创建表的时候通过从别的表中查询出相应的记录并插入到所创建的表中。

    1、从本地文件系统中导入数据到Hive表
        先在Hive里面创建好表，如下：
        hive> create table wyp
            > (id int, name string,
            > age int, tel string)
            > ROW FORMAT DELIMITED
            > FIELDS TERMINATED BY '\t'
            > STORED AS TEXTFILE;

        这个表很简单，只有四个字段，具体含义我就不解释了。本地文件系统里面有个/home/wyp/wyp.txt文件，内容如下：
        [wyp@master ~]$ cat wyp.txt
        1       wyp     25      13188888888888
        2       test    30      13888888888888
        3       zs      34      899314121

        wyp.txt文件中的数据列之间是使用\t分割的，可以通过下面的语句将这个文件里面的数据导入到wyp表里面，操作如下：
        hive> load data local inpath 'wyp.txt' into table wyp;

        这样就将wyp.txt里面的内容导入到wyp表里面去了，可以到wyp表的数据目录下查看，如下命令：
        hive> dfs -ls /user/hive/warehouse/wyp ;

        需要注意的是：
        和我们熟悉的关系型数据库不一样，Hive现在还不支持在insert语句里面直接给出一组记录的文字形式，也就是说，Hive并不支持INSERT INTO …. VALUES形式的语句。

    2、HDFS上导入数据到Hive表
        从本地文件系统中将数据导入到Hive表的过程中，其实是先将数据临时复制到HDFS的一个目录下（典型的情况是复制到上传用户的HDFS home目录下,比如/home/wyp/），
        然后再将数据从那个临时目录下移动（注意，这里说的是移动，不是复制！）到对应的Hive表的数据目录里面。既然如此，那么Hive肯定支持将数据直接从HDFS上的一个
        目录移动到相应Hive表的数据目录下，假设有下面这个文件/home/wyp/add.txt，具体的操作如下：
        [wyp@master /home/q/hadoop-2.2.0]$ bin/hadoop fs -cat /home/wyp/add.txt
        5       wyp1    23      131212121212
        6       wyp2    24      134535353535
        7       wyp3    25      132453535353
        8       wyp4    26      154243434355

        上面是需要插入数据的内容，这个文件是存放在HDFS上/home/wyp目录（和一中提到的不同，一中提到的文件是存放在本地文件系统上）里面，我们可以通过下面的命令将
        这个文件里面的内容导入到Hive表中，具体操作如下：
        hive> load data inpath '/home/wyp/add.txt' into table wyp;

        请注意load data inpath ‘/home/wyp/add.txt’ into table wyp;里面是没有local这个单词的，这个是和一中的区别。

    3、从别的表中查询出相应的数据并导入到Hive表中

        hive> insert into table test
            > partition (age='25')
            > select id, name, tel
            > from wyp;

        如果目标表（test）中不存在分区字段，可以去掉partition (age=’25′)语句。当然，我们也可以在select语句里面通过使用分区值来动态指明分区：
        hive> set hive.exec.dynamic.partition.mode=nonstrict;
        hive> insert into table test
            > partition (age)
            > select id, name,
            > tel, age
            > from wyp;

        这种方法叫做动态分区插入，但是Hive中默认是关闭的，所以在使用前需要先把hive.exec.dynamic.partition.mode设置为nonstrict。当然，
        Hive也支持insert overwrite方式来插入数据，从字面我们就可以看出，overwrite是覆盖的意思，是的，执行完这条语句的时候，相应数据
        目录下的数据将会被覆盖！而insert into则不会，注意两者之间的区别。例子如下：
        hive> insert overwrite table test
            > PARTITION (age)
            > select id, name, tel, age
            > from wyp;

        更可喜的是，Hive还支持多表插入，什么意思呢？在Hive中，我们可以把insert语句倒过来，把from放在最前面，它的执行效果和放在后面是一样的，如下：
        hive> from wyp
            > insert into table test
            > partition(age)
            > select id, name, tel, age
            > insert into table test3
            > select id, name
            > where age>25;
        可以在同一个查询中使用多个insert子句，这样的好处是我们只需要扫描一遍源表就可以生成多个不相交的输出。这个很酷吧！

    4、在创建表的时候通过从别的表中查询出相应的记录并插入到所创建的表中
        在实际情况中，表的输出结果可能太多，不适于显示在控制台上，这时候，将Hive的查询输出结果直接存在一个新的表中是非常方便的，
        我们称这种情况为CTAS（create table .. as select）如下：
        hive> create table test4
            > as
            > select id, name, tel
            > from wyp;
        数据就插入到test4表中去了，CTAS操作是原子的，因此如果select查询由于某种原因而失败，新表是不会创建的！
